{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.width', 500)\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import igraph as ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not in use, takes too long to load even part of entire graph from edge list\n",
    "def create_igraph_graph_from_edgelist(fn, nrows=None):\n",
    "    print(\"reading edges ... \", end='')\n",
    "    start = time.time()\n",
    "    df = pd.read_csv(fn, nrows=nrows)\n",
    "    end = time.time()\n",
    "    print(\"read {:_} lines (took {:.1f}s)\".format(len(df), (end-start)))\n",
    "    print(\"creating graph ... \", end='')\n",
    "    start = time.time()\n",
    "    g = ig.Graph.TupleList(df.values)\n",
    "    end = time.time()\n",
    "    print(\"created graph with {:_} nodes and {:_} edges (took {:.1f}s)\".format(len(g.vs), len(g.es), (end-start)))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not in use, takes too long to load even part of entire graph from edge list\n",
    "def create_networkx_graph_from_edgelist(fn, nrows=None):\n",
    "    print(\"reading edges ... \", end='')\n",
    "    start = time.time()\n",
    "    edges_df = pd.read_csv(fn, nrows=nrows)\n",
    "    end = time.time()\n",
    "    print(\"read {:_} lines (took {:.1f}s)\".format(len(edges_df), (end-start)))\n",
    "    print(\"creating graph ... \", end='')\n",
    "    start = time.time()\n",
    "    g = nx.from_pandas_edgelist(edges_df, source='source', target='target')\n",
    "    end = time.time()\n",
    "    print(\"created graph with {:_} nodes and {:_} edges (took {:.1f}s)\".format(len(g.nodes), len(g.edges), (end-start)))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_dataframe():\n",
    "    # Import dataset from tsv file\n",
    "    dataset_fn = \"dataset/TweetsCOV19.tsv\"\n",
    "    header = [\"Tweet Id\", \"Username\", \"Timestamp\", \"Followers\", \"Friends\", \"Retweets\", \"Favorites\", \"Entities\", \"Sentiment\", \"Mentions\", \"Hashtags\", \"URLs\", \"EXTRA\"]\n",
    "    dtype = {\"Tweet Id\":\"string\", \"Username\":\"string\", \"Timestamp\":\"string\", \"Followers\":int, \"Friends\":int, \"Retweets\":int, \"Favorites\":int, \"Entities\":\"string\", \"Sentiment\":\"string\", \"Mentions\":\"string\", \"Hashtags\":\"string\", \"URLs\":\"string\", \"EXTRA\":\"string\"}\n",
    "    print(\"Importing dataset from tsv file ...\", end='')\n",
    "    start = time.time()\n",
    "    df = pd.read_csv(dataset_fn, sep='\\t', names=header, on_bad_lines='warn', dtype=dtype)\n",
    "    end = time.time()\n",
    "    print(\"read {:_} lines (took {:.1f}s)\".format(len(df), end-start))\n",
    "    df.set_index('Tweet Id', inplace=True)\n",
    "\n",
    "    # Convert timestamp column to Timestamp object\n",
    "    print(\"Converting timestamp column\")\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%a %b %d %H:%M:%S %z %Y')\n",
    "\n",
    "    # Filter columns and timestamp\n",
    "    print(\"Filtering desired columns and between desired dates ... \", end='')\n",
    "    dff = df[[\"Username\", \"Timestamp\", \"Sentiment\", \"Hashtags\"]]\n",
    "    start_date =    pd.to_datetime('2019-12-01 00:00:00 +0000')\n",
    "    end_date =      pd.to_datetime('2020-03-01 00:00:00 +0000')\n",
    "    dff = dff[(dff['Timestamp'] >= start_date) & (dff['Timestamp'] < end_date)]\n",
    "    print(\"{:_} rows in dataframe\".format(len(df)))\n",
    "\n",
    "    # Parse hashtags tab into array\n",
    "    print(\"Parsing hashtags and positive/negative sentiments\")\n",
    "    dff['Hashtags'] = dff['Hashtags'].str.split().apply(lambda x: [name for name in x if name != \"null;\"] if isinstance(x, list) else [])\n",
    "\n",
    "    # Split positive and negative sentiments into own columns (and convert to int type)\n",
    "    dff[['Sentiment_pos', 'Sentiment_neg']] = dff['Sentiment'].str.split(\" \", expand=True)\n",
    "    dff['Sentiment_pos'], dff['Sentiment_neg'] = dff['Sentiment_pos'].astype(int), dff['Sentiment_neg'].astype(int)\n",
    "    dff.drop(\"Sentiment\", axis=1, inplace=True)\n",
    "\n",
    "    # Filter rows with mentions (and less that outlier mentions)\n",
    "    print(\"filtering for tweets that contain hashtags ... \", end='')\n",
    "    ht = dff[dff['Hashtags'].apply(lambda x: len(x) > 0 and len(x) < 60)]\n",
    "    print(\"{:_} rows in dataframe\".format(len(df)))\n",
    "\n",
    "    return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_progess(total, progress, text='progress:', inc=1):\n",
    "    a, b = progress, total\n",
    "    perc = ((a+1) / b * 100)\n",
    "    if a%inc == 0:\n",
    "        print(\"\\r {} {:_}/{:_} ({:.5f}%)\".format( text, (a+1), b, perc ), end='')\n",
    "    a += 1\n",
    "    return a, perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "edges_fn = \"../data/edges.csv\"\n",
    "edges_total = 684_732_453 # hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read edges to dataframe\n",
    "perc = 100\n",
    "g = create_igraph_graph_from_edgelist(edges_fn, nrows=int(edges_total*perc/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display degree centrality distribution\n",
    "degree = g.degree()\n",
    "print(np.mean(degree))\n",
    "print(len(degree))\n",
    "plt.hist(degree, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs = g.connected_components()\n",
    "print(\"found {:_} connected components\".format(len(ccs)))\n",
    "print(ccs.sizes())\n",
    "print(max(ccs.sizes()))\n",
    "i = ccs.sizes().index(max(ccs.sizes()))\n",
    "print(i)\n",
    "print(\"largest connected component has {:_} nodes\".format(len(ccs[i])))\n",
    "#sg = g.subgraph(ccs[i])\n",
    "#print(sg)\n",
    "#print(ccs[i])\n",
    "#print(ccs.membership)\n",
    "#for cc in ccs:\n",
    "#    print(len(cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make subgraph from largest connected component\n",
    "sg = g.subgraph(ccs[i])\n",
    "print(\"largest connected component has {:_} edges\".format(len(sg.es)))\n",
    "\n",
    "average_shortest_path = np.mean(sg.shortest_paths())\n",
    "print(\"Average shortest path in largest connected component:\", average_shortest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average shortest path\n",
    "average_shortest_path = np.mean(g.shortest_paths())\n",
    "print(average_shortest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UP TO HERE!!! ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweets dataframe\n",
    "ht = get_tweets_dataframe()\n",
    "df_data_usage = ht.memory_usage(deep=True).sum()\n",
    "print(\"rows:    {:_}\".format( ht.shape[0] ))\n",
    "print(\"size mb: {:_}\".format( int(df_data_usage /(1024**2)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df with only tweets in graph g\n",
    "ig = ht[ht['Tweet Id'].apply(lambda x: x in g)]\n",
    "df_data_usage = ig.memory_usage(deep=True).sum()\n",
    "print(\"rows:    {:_}\".format( ig.shape[0] ))\n",
    "print(\"size mb: {:_}\".format( int(df_data_usage /(1024**2)) ))\n",
    "print(ig.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting node values\n",
    "print(\"\")\n",
    "nodes_handled = 0\n",
    "for n in g.nodes:\n",
    "    try:\n",
    "        row = ht.loc[n]\n",
    "    except:\n",
    "        print(n)\n",
    "        print(nodes_handled)\n",
    "        break\n",
    "    g.nodes[n]['timestamp'] = row['Timestamp']\n",
    "    g.nodes[n]['positive_sentiment'] = row['Sentiment_pos']\n",
    "    g.nodes[n]['negative_sentiment'] = row['Sentiment_neg']\n",
    "    nodes_handled, perc = track_progess(len(g.nodes), nodes_handled, text='nodes handled:', inc=25)\n",
    "    #if perc > 2: break\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select nodes before date and make subgraph\n",
    "n = list(g.nodes)[0]\n",
    "print(g.nodes[n])\n",
    "ts = pd.to_datetime(\"2020-01-03 00:00:00 +0000\")\n",
    "before_nodes = [ n for n in g.nodes if g.nodes[n]['timestamp'] < ts ]\n",
    "print(len(before_nodes))\n",
    "sg = g.subgraph(before_nodes)\n",
    "print(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix\n",
    "matrix = nx.adjacency_matrix(g)\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connected components\n",
    "ccs = list(nx.connected_components(g))\n",
    "print(len(ccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cc in ccs:\n",
    "    print(len(cc))\n",
    "    if len(cc) < 18:\n",
    "        print(cc)\n",
    "        sg = g.subgraph(cc)\n",
    "        print(sg)\n",
    "        nx.draw(sg)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree centralities histogram\n",
    "degree_centralities = nx.degree_centrality(g)\n",
    "values = list(degree_centralities.values())\n",
    "plt.hist(values, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_each_hashtag(df):\n",
    "    array_col = \"Hashtags\"\n",
    "    dicti = {}\n",
    "    print(\"Getting list of ids per hashtag ...\")\n",
    "    i = 0\n",
    "    for _, row in df.iterrows():\n",
    "        i += 1\n",
    "        perc = (i) / len(df) * 100\n",
    "        print(\"\\r {:_}/{:_} ({:.1f}%)\".format(i, len(df), perc), end='')\n",
    "        for term in set(row[array_col]):\n",
    "            dicti[term] = dicti.get(term, []) + [row.name]\n",
    "    print(\"\\nDone.\")\n",
    "    print(\"Found {:_} unique hashtags\".format(len(dicti)))\n",
    "    dicti = { k: v for k, v in dicti.items() if len(v) > 1 }\n",
    "    print(\"Found {:_} hashtags with more than 1 associated tweet\".format(len(dicti)))\n",
    "    return dicti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_from_hashtags(ht_dict):\n",
    "    print(\"Creating edges from {} hashtags ...\".format(len(ht_dict)))\n",
    "    g = nx.Graph()\n",
    "    for _, ids in ht_dict.items():\n",
    "        for i in range(len(ids)):\n",
    "            for j in range(i+1, len(ids)):\n",
    "                g.add_edge(ids[i], ids[j])\n",
    "    print(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BACK TO DATAFRAMES ###\n",
    "df = get_tweets_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = list(df['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(timestamps))\n",
    "print(type(timestamps[0]))\n",
    "print(max(timestamps))\n",
    "print(min(timestamps))\n",
    "ts_start, ts_end = min(timestamps), max(timestamps)\n",
    "ts_inc = (ts_end - ts_start) / 100\n",
    "window_start = ts_start\n",
    "window_end = window_start + 1*ts_inc\n",
    "sel = df[(window_start <= df['Timestamp']) & (df['Timestamp'] < window_end)]\n",
    "print(sel.shape)\n",
    "#sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_dict = get_tweets_for_each_hashtag(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of edges that will be created\n",
    "from math import comb\n",
    "edges_n = sum([ comb(len(v),2) for v in list(ht_dict.values())[:-1] ])\n",
    "print(\"Number of edges that will be created: {:_}\".format(edges_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make graph from hashtags\n",
    "g = make_graph_from_hashtags(ht_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(g, \"../data/increments/edgelist_1.csv\", data=[\"source\", \"target\"])\n",
    "#nx.write_edgelist(g, \"test.csv\", data=[\"source\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes = list(g.nodes)[:]\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "#nx.draw_networkx(g.subgraph(nodes), with_labels=False, ax=ax)\n",
    "nx.draw_networkx(g, with_labels=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increments = 100\n",
    "ts_start, ts_end = min(timestamps), max(timestamps)\n",
    "ts_inc = (ts_end - ts_start) / 100\n",
    "for i in range(increments):\n",
    "    window_start = ts_start\n",
    "    window_end = window_start + (i+1)*ts_inc\n",
    "    sel = df[(window_start <= df['Timestamp']) & (df['Timestamp'] < window_end)]\n",
    "    print(sel)\n",
    "    ht_dict = get_tweets_for_each_hashtag(sel)\n",
    "    g = make_graph_from_hashtags(ht_dict)\n",
    "    nx.write_edgelist(g, \"../data/increments/edgelist_{}.csv\".format(i+1), data=[\"source\", \"target\"])\n",
    "    \n",
    "    input(\"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
