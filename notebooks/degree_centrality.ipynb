{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.width', 500)\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import igraph as ig\n",
    "from fun.fun import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_dataframe():\n",
    "    # Import dataset from tsv file\n",
    "    dataset_fn = \"../dataset/TweetsCOV19.tsv\"\n",
    "    header = [\"Tweet Id\", \"Username\", \"Timestamp\", \"Followers\", \"Friends\", \"Retweets\", \"Favorites\", \"Entities\", \"Sentiment\", \"Mentions\", \"Hashtags\", \"URLs\", \"EXTRA\"]\n",
    "    dtype = {\"Tweet Id\":\"string\", \"Username\":\"string\", \"Timestamp\":\"string\", \"Followers\":int, \"Friends\":int, \"Retweets\":int, \"Favorites\":int, \"Entities\":\"string\", \"Sentiment\":\"string\", \"Mentions\":\"string\", \"Hashtags\":\"string\", \"URLs\":\"string\", \"EXTRA\":\"string\"}\n",
    "    print(\"Importing dataset from tsv file ...\", end='')\n",
    "    start = time.time()\n",
    "    df = pd.read_csv(dataset_fn, sep='\\t', names=header, on_bad_lines='warn', dtype=dtype)\n",
    "    end = time.time()\n",
    "    print(\"read {:_} lines (took {:.1f}s)\".format(len(df), end-start))\n",
    "    df.set_index('Tweet Id', inplace=True)\n",
    "\n",
    "    # Convert timestamp column to Timestamp object\n",
    "    print(\"Converting timestamp column\")\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%a %b %d %H:%M:%S %z %Y')\n",
    "\n",
    "    # Filter columns and timestamp\n",
    "    print(\"Filtering desired columns and between desired dates ... \", end='')\n",
    "    dff = df[[\"Username\", \"Timestamp\", \"Sentiment\", \"Hashtags\"]]\n",
    "    start_date =    pd.to_datetime('2019-12-01 00:00:00 +0000')\n",
    "    end_date =      pd.to_datetime('2020-03-01 00:00:00 +0000')\n",
    "    dff = dff[(dff['Timestamp'] >= start_date) & (dff['Timestamp'] < end_date)]\n",
    "    print(\"{:_} rows in dataframe\".format(len(df)))\n",
    "\n",
    "    # Parse hashtags tab into array\n",
    "    print(\"Parsing hashtags and positive/negative sentiments\")\n",
    "    dff['Hashtags'] = dff['Hashtags'].str.split().apply(lambda x: [name for name in x if name != \"null;\"] if isinstance(x, list) else [])\n",
    "\n",
    "    # Split positive and negative sentiments into own columns (and convert to int type)\n",
    "    dff[['Sentiment_pos', 'Sentiment_neg']] = dff['Sentiment'].str.split(\" \", expand=True)\n",
    "    dff['Sentiment_pos'], dff['Sentiment_neg'] = dff['Sentiment_pos'].astype(int), dff['Sentiment_neg'].astype(int)\n",
    "    dff.drop(\"Sentiment\", axis=1, inplace=True)\n",
    "\n",
    "    # Filter rows with mentions (and less that outlier mentions)\n",
    "    print(\"filtering for tweets that contain hashtags ... \", end='')\n",
    "    ht = dff[dff['Hashtags'].apply(lambda x: len(x) > 0 and len(x) < 60)]\n",
    "    print(\"{:_} rows in dataframe\".format(len(df)))\n",
    "\n",
    "    return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "edges_fn = \"../data/edges.csv\"\n",
    "edges_total = 684_732_453 # hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read edges to dataframe\n",
    "perc = 100\n",
    "nrows=int(edges_total*perc/100)\n",
    "print(\"reading edges ... \", end='')\n",
    "start = time.time()\n",
    "df = pd.read_csv(edges_fn, nrows=nrows)\n",
    "end = time.time()\n",
    "print(\"read {:_} lines (took {:.1f}s)\".format(len(df), (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['source'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read edges to dataframe\n",
    "perc = 100\n",
    "nrows=int(edges_total*perc/100)\n",
    "print(\"reading edges ... \", end='')\n",
    "start = time.time()\n",
    "df = pd.read_csv(edges_fn, nrows=nrows, dtype={'source':'int32', 'target':'int32'})\n",
    "end = time.time()\n",
    "print(\"read {:_} lines (took {:.1f}s)\".format(len(df), (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating graph ... \", end='')\n",
    "start = time.time()\n",
    "g = ig.Graph.TupleList(df.values)\n",
    "end = time.time()\n",
    "print(\"created graph with {:_} nodes and {:_} edges (took {:.1f}s)\".format(len(g.vs), len(g.es), (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality of graph via edge list\n",
    "degree_cent = {}\n",
    "for i, node in enumerate(pd.concat([df['source'], df['target']])):\n",
    "    degree_cent[node] = degree_cent.get(node, 0) + 1\n",
    "    _, perc = track_progess(len(df['source'])*2, i, inc=25)\n",
    "print(\"\\nDone.\")\n",
    "print(len(degree_cent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get estimate for nodelist representation of grpah\n",
    "size = 0\n",
    "base = 19\n",
    "for v in degree_cent.values():\n",
    "    size += base * (1 + v)\n",
    "print(\"{:_}\".format(size/1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = degree_cent.values()\n",
    "plt.hist(values, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power_law_fit(values, title='power law fit check', figsize=(5,5), ci=0.95):\n",
    "    v_min, v_max = min(values), max(values)\n",
    "    X = np.logspace(np.log10(v_min), np.log10(v_max), 40) # generate log distributed x values\n",
    "    Y = [len([v for v in values if v <= x]) for x in X] # cumulative sum\n",
    "\n",
    "    lnX, lnY = np.log(X), np.log(Y)\n",
    "\n",
    "    \"\"\" regr = LinearRegression().fit(lnX.reshape(-1,1), lnY)\n",
    "    lnY_pred = regr.predict(lnX.reshape(-1,1)) \"\"\"\n",
    "\n",
    "    lnX_con = sm.add_constant(lnX)\n",
    "    lr = sm.OLS(lnY, lnX_con).fit()\n",
    "    y_int, grad = lr.params\n",
    "    conf_interval = lr.conf_int(1-ci)\n",
    "    (y_int_lower, y_int_upper), _ = conf_interval\n",
    "    lnY_pred = lr.predict(lnX_con)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(lnX, lnY, 'r.', label='samples')\n",
    "    ax.plot(lnX, y_int + grad*lnX, 'g', label='regression line')\n",
    "    ax.plot(lnX, y_int_lower + grad*lnX, 'b', label='lower ci ({})'.format(ci))\n",
    "    ax.plot(lnX, y_int_upper + grad*lnX, 'b', label='upper ci ({})'.format(ci))\n",
    "    #ax.plot(lnX, lnY_pred, 'p', label='pred'.format(ci))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.ylabel('log(n)')\n",
    "    plt.xlabel('log(p_k)')\n",
    "\n",
    "    plt.legend(loc=0)\n",
    "    leg = plt.gca().get_legend()\n",
    "    ltext = leg.get_texts()\n",
    "    plt.setp(ltext, fontsize=10)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_power_law_fit(values, title='1% of whole graph')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
